/Users/mluser/Desktop/projects/englist_2_telugu/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
  warnings.warn(warn_msg)
Using device:  mps
Batch: 100, Global Step: 0, Loss: 7.8007, Time: 42.46973395347595
Batch: 200, Global Step: 0, Loss: 7.3971, Time: 80.00344395637512
Batch: 300, Global Step: 0, Loss: 6.4508, Time: 118.03506398200989
Batch: 400, Global Step: 0, Loss: 7.4795, Time: 156.61175203323364
Batch: 500, Global Step: 0, Loss: 6.7266, Time: 194.14562487602234
Batch: 600, Global Step: 0, Loss: 6.4633, Time: 231.74513006210327
Batch: 700, Global Step: 0, Loss: 6.5177, Time: 269.72948384284973
Batch: 800, Global Step: 0, Loss: 5.5011, Time: 307.3838701248169
Batch: 900, Global Step: 0, Loss: 6.6758, Time: 346.53984808921814
Batch: 1000, Global Step: 0, Loss: 6.2391, Time: 385.4832589626312
Batch: 1100, Global Step: 0, Loss: 5.9275, Time: 424.44509506225586
Batch: 1200, Global Step: 0, Loss: 5.7033, Time: 463.44745683670044
Batch: 1300, Global Step: 0, Loss: 5.6304, Time: 502.30719900131226
Batch: 1400, Global Step: 0, Loss: 5.3915, Time: 541.279904127121
Batch: 1500, Global Step: 0, Loss: 4.7310, Time: 580.3981039524078
Batch: 1600, Global Step: 0, Loss: 4.6901, Time: 619.0347788333893
Batch: 1700, Global Step: 0, Loss: 4.6510, Time: 657.7834599018097
Batch: 1800, Global Step: 0, Loss: 4.5353, Time: 696.5498068332672
Batch: 1900, Global Step: 0, Loss: 4.6946, Time: 735.1732459068298
Batch: 2000, Global Step: 0, Loss: 4.4526, Time: 773.9453709125519
Batch: 2100, Global Step: 0, Loss: 5.5149, Time: 812.6436591148376
Batch: 2200, Global Step: 0, Loss: 4.5872, Time: 851.9733011722565
Batch: 2300, Global Step: 0, Loss: 4.2824, Time: 889.2417318820953
Batch: 2400, Global Step: 0, Loss: 3.6673, Time: 927.696692943573
Batch: 2500, Global Step: 0, Loss: 3.7649, Time: 966.8524758815765
Batch: 2600, Global Step: 0, Loss: 4.6261, Time: 1006.4200410842896
Batch: 2700, Global Step: 0, Loss: 3.8077, Time: 1045.3292129039764
Batch: 2800, Global Step: 0, Loss: 3.4262, Time: 1082.7320311069489
Batch: 2900, Global Step: 0, Loss: 3.7605, Time: 1119.981143951416
Batch: 3000, Global Step: 0, Loss: 2.5047, Time: 1157.2762608528137
Batch: 3100, Global Step: 0, Loss: 3.1630, Time: 1194.5484359264374
Batch: 3200, Global Step: 0, Loss: 3.4687, Time: 1231.9204161167145
Batch: 3300, Global Step: 0, Loss: 3.8065, Time: 1269.132411956787
Batch: 3400, Global Step: 0, Loss: 3.0600, Time: 1306.3629400730133
Batch: 3500, Global Step: 0, Loss: 3.0337, Time: 1344.3409130573273
Batch: 3600, Global Step: 0, Loss: 3.0225, Time: 1382.8606758117676
Batch: 3700, Global Step: 0, Loss: 3.6491, Time: 1421.7368190288544
Batch: 3800, Global Step: 0, Loss: 3.5753, Time: 1460.3593800067902
Batch: 3900, Global Step: 0, Loss: 3.4601, Time: 1498.845673084259
Batch: 4000, Global Step: 0, Loss: 2.7298, Time: 1537.6762700080872
Batch: 4100, Global Step: 0, Loss: 2.4390, Time: 1577.5283160209656
Batch: 4200, Global Step: 0, Loss: 2.4460, Time: 1615.9085059165955
Batch: 4300, Global Step: 0, Loss: 2.8733, Time: 1654.0555458068848
Batch: 4400, Global Step: 0, Loss: 2.4522, Time: 1691.3076939582825
Batch: 4500, Global Step: 0, Loss: 3.2782, Time: 1728.7773859500885
Batch: 4600, Global Step: 0, Loss: 2.7964, Time: 1766.3550250530243
Batch: 4700, Global Step: 0, Loss: 3.0682, Time: 1803.8914670944214
Batch: 4800, Global Step: 0, Loss: 2.5675, Time: 1841.482881784439
Batch: 4900, Global Step: 0, Loss: 2.3629, Time: 1879.122162103653
Batch: 5000, Global Step: 0, Loss: 2.7170, Time: 1916.9442870616913
Batch: 5100, Global Step: 0, Loss: 2.1784, Time: 1955.3253870010376
Batch: 5200, Global Step: 0, Loss: 2.2497, Time: 1995.2058508396149
Batch: 5300, Global Step: 0, Loss: 2.5204, Time: 2035.558912038803
Batch: 5400, Global Step: 0, Loss: 2.4899, Time: 2075.3163039684296
Batch: 5500, Global Step: 0, Loss: 2.4603, Time: 2114.910945892334
Batch: 5600, Global Step: 0, Loss: 2.5926, Time: 2154.17405295372
Batch: 5700, Global Step: 0, Loss: 2.4768, Time: 2192.1757519245148
Batch: 5800, Global Step: 0, Loss: 2.3968, Time: 2230.54647397995
Batch: 5900, Global Step: 0, Loss: 2.7939, Time: 2270.3374931812286
Batch: 6000, Global Step: 0, Loss: 2.3570, Time: 2308.402219772339
Batch: 6100, Global Step: 0, Loss: 2.0716, Time: 2345.453473806381
Batch: 6200, Global Step: 0, Loss: 2.6776, Time: 2383.099942922592
Batch: 6300, Global Step: 0, Loss: 2.1687, Time: 2420.4994671344757
Batch: 6400, Global Step: 0, Loss: 2.3141, Time: 2457.8978881835938
Batch: 6500, Global Step: 0, Loss: 1.9526, Time: 2495.3808300495148
Batch: 6600, Global Step: 0, Loss: 2.7546, Time: 2532.71467089653
Batch: 6700, Global Step: 0, Loss: 2.3202, Time: 2570.892516851425
Batch: 6800, Global Step: 0, Loss: 2.0817, Time: 2608.4658048152924
Batch: 6900, Global Step: 0, Loss: 2.5498, Time: 2645.53174495697
Batch: 7000, Global Step: 0, Loss: 2.0846, Time: 2683.1311478614807
Batch: 7100, Global Step: 0, Loss: 2.4933, Time: 2720.364711999893
Batch: 7200, Global Step: 0, Loss: 2.1449, Time: 2757.767643928528
Batch: 7300, Global Step: 0, Loss: 2.1925, Time: 2795.1449089050293
Batch: 7400, Global Step: 0, Loss: 2.6764, Time: 2832.41002702713
Batch: 7500, Global Step: 0, Loss: 2.3536, Time: 2872.0538759231567
Batch: 7600, Global Step: 0, Loss: 1.8683, Time: 2911.5784199237823
Batch: 7700, Global Step: 0, Loss: 1.9129, Time: 2951.146934747696
Batch: 7800, Global Step: 0, Loss: 1.8554, Time: 2990.746826171875
Batch: 7900, Global Step: 0, Loss: 2.1508, Time: 3030.3175439834595
Batch: 8000, Global Step: 0, Loss: 1.9288, Time: 3069.76459980011
Batch: 8100, Global Step: 0, Loss: 1.8111, Time: 3109.1583881378174
Batch: 8200, Global Step: 0, Loss: 2.0086, Time: 3147.541883945465
Batch: 8300, Global Step: 0, Loss: 2.1298, Time: 3184.623805999756
Batch: 8400, Global Step: 0, Loss: 2.0045, Time: 3222.0013489723206
Batch: 8500, Global Step: 0, Loss: 1.6087, Time: 3259.1043038368225
Batch: 8600, Global Step: 0, Loss: 2.0581, Time: 3296.3126978874207
Batch: 8700, Global Step: 0, Loss: 1.8806, Time: 3333.629201889038
Batch: 8800, Global Step: 0, Loss: 2.1894, Time: 3370.9038059711456
Batch: 8900, Global Step: 0, Loss: 1.9675, Time: 3408.2779338359833
Batch: 9000, Global Step: 0, Loss: 1.9291, Time: 3446.562355995178
Batch: 9100, Global Step: 0, Loss: 1.8278, Time: 3484.987897872925
Batch: 9200, Global Step: 0, Loss: 1.8905, Time: 3523.6229078769684
Batch: 9300, Global Step: 0, Loss: 1.8557, Time: 3562.558230161667
Batch: 9400, Global Step: 0, Loss: 1.8794, Time: 3601.2368609905243
Batch: 9500, Global Step: 0, Loss: 1.9950, Time: 3640.3253688812256
Batch: 9600, Global Step: 0, Loss: 2.1133, Time: 3679.1829459667206
Batch: 9700, Global Step: 0, Loss: 1.9192, Time: 3718.0395538806915
Batch: 9800, Global Step: 0, Loss: 1.5586, Time: 3757.7385189533234
Batch: 9900, Global Step: 0, Loss: 1.6522, Time: 3796.4813997745514
Batch: 10000, Global Step: 0, Loss: 1.9729, Time: 3835.0300199985504
Batch: 10100, Global Step: 0, Loss: 1.9627, Time: 3874.0552690029144
Batch: 10200, Global Step: 0, Loss: 1.9666, Time: 3912.8125309944153
Batch: 10300, Global Step: 0, Loss: 1.8678, Time: 3951.6729078292847
Batch: 10400, Global Step: 0, Loss: 2.2046, Time: 3990.454262971878
Batch: 10500, Global Step: 0, Loss: 1.8565, Time: 4029.326302051544
Batch: 10600, Global Step: 0, Loss: 1.7860, Time: 4067.9324159622192
Batch: 10700, Global Step: 0, Loss: 1.7184, Time: 4106.768567800522
Batch: 10800, Global Step: 0, Loss: 1.6410, Time: 4145.293152809143
Batch: 10900, Global Step: 0, Loss: 1.8517, Time: 4183.953680038452
Batch: 11000, Global Step: 0, Loss: 1.8125, Time: 4222.690505027771
Batch: 11100, Global Step: 0, Loss: 2.0354, Time: 4261.251032829285
Batch: 11200, Global Step: 0, Loss: 1.6303, Time: 4300.166785001755
Batch: 11300, Global Step: 0, Loss: 1.8605, Time: 4338.957545995712
Batch: 11400, Global Step: 0, Loss: 1.7462, Time: 4377.58734703064
Batch: 11500, Global Step: 0, Loss: 1.9678, Time: 4416.407112121582
Batch: 11600, Global Step: 0, Loss: 1.6215, Time: 4454.9655430316925
Batch: 11700, Global Step: 0, Loss: 1.8644, Time: 4493.6780779361725
Batch: 11800, Global Step: 0, Loss: 1.9516, Time: 4532.337283849716
Batch: 11900, Global Step: 0, Loss: 1.7900, Time: 4570.954049110413
Batch: 12000, Global Step: 0, Loss: 1.6715, Time: 4609.654799938202
Batch: 12100, Global Step: 0, Loss: 2.7634, Time: 4648.7919681072235
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 15 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
